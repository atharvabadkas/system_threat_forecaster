
# 🛡️ Malware Prediction Project

This project is a part of the MLP Kaggle competition aimed at predicting whether a machine is infected by malware, based on a wide range of system-level telemetry features. The solution involves a full pipeline starting from data loading and cleaning, to EDA, modeling with multiple algorithms, evaluation, and generating Kaggle submission files.

---

## 📌 Problem Statement

The goal is to predict a system’s probability of getting infected by various families of malware using data generated by antivirus telemetry logs. Each row in the dataset corresponds to a machine uniquely identified by `MachineID`. The `target` column indicates whether malware was detected (`1`) or not (`0`).

---

## 📂 Dataset Description

| File                   | Description                                                  |
|------------------------|--------------------------------------------------------------|
| `train.csv`            | Training dataset with machine features and target labels.    |
| `test.csv`             | Test dataset where predictions are to be made.               |
| `sample_submission.csv`| Format required for Kaggle submission.                       |

- **Target**: Binary classification — 1 if malware is detected, 0 otherwise.
- **Evaluation Metric**: `accuracy_score` between predicted labels and true labels.

---

## 🧪 Exploratory Data Analysis (EDA)

Conducted in `kaggle_eda.py`, this phase covers:

- Identifying and visualizing missing values using `missingno`
- Understanding feature distributions and relationships
- Analyzing the balance of target variable
- Correlation matrix and heatmaps
- Initial feature importance based on domain and statistical relevance

---

## ⚙️ Modeling Approach

We experimented with three high-performance ensemble-based models:

### 🔍 1. Random Forest
Implemented in `random_forest_model.py`:
- Suitable as a baseline model
- Handles high-dimensional data
- Easy interpretability

### 🚀 2. XGBoost
Implemented in `xgboost_model.py`:
- Gradient boosting framework
- Efficient with regularization support
- Handles missing values internally

### ⚡ 3. LightGBM
Implemented in `lightgbm_model.py`:
- Gradient boosting using histogram-based learning
- Faster and more efficient on large datasets
- Supports categorical features natively

Each model performs:
- Data preprocessing (encoding, scaling, imputation)
- Splitting data into train-validation sets
- Training and evaluation using `accuracy_score`
- Generating `submission.csv` compatible with Kaggle

---

## 🔬 Model Performance

| Model           | Validation Accuracy | Approx. Kaggle Score |
|-----------------|---------------------|----------------------|
| Random Forest   | 0.56 – 0.58         | ~0.55                |
| XGBoost         | 0.60 – 0.62         | ~0.61                |
| LightGBM        | 0.63 – 0.65         | ~0.64 (best)         |

> Target model accuracy > 0.70 (training) to achieve ~0.63+ Kaggle score.

---

## 🧰 Tech Stack Used

- **Python 3.8+**
- **Libraries:**
  - Data: `pandas`, `numpy`
  - Visualization: `matplotlib`, `seaborn`, `missingno`
  - Modeling: `scikit-learn`, `xgboost`, `lightgbm`
  - Utilities: `pickle`, `re`, `scipy`

> See `requirements.txt` for full library versions.

---

## 🏗️ Project Structure

```plaintext
.
├── kaggle_eda.py                # Exploratory Data Analysis
├── lightgbm_model.py            # LightGBM training and prediction
├── xgboost_model.py             # XGBoost training and prediction
├── random_forest_model.py       # Random Forest training and prediction
├── models.py                    # Common modeling utilities
├── train.csv                    # Labeled training data
├── test.csv                     # Test data for predictions
├── requirements.txt             # Required Python packages
├── README.md                    # Project documentation
```

---

## ▶️ How to Run

1. Upload `train.csv` and `test.csv` to Kaggle or your local notebook.
2. Run `kaggle_eda.py` to analyze data and understand distributions.
3. Run each of the model scripts to evaluate performance:
   - `random_forest_model.py`
   - `xgboost_model.py`
   - `lightgbm_model.py`
4. The script will generate `submission.csv` for Kaggle upload.

---

## ✅ Evaluation Criteria

To pass the project successfully:

- [x] Load and process dataset correctly
- [x] Conduct insightful EDA
- [x] Handle missing values, feature encoding, and scaling
- [x] Use at least 3 ML models and tune hyperparameters
- [x] Score > 0.60 on validation
- [x] Produce Kaggle-compatible submission
- [x] Maintain clean, commented code

---

## 📌 Notes and Recommendations

- Avoid using libraries like TensorFlow, PyTorch, NLTK, etc.
- Use proper validation (e.g., `train_test_split`, `StratifiedKFold`)
- Consider feature engineering if performance stalls
- Maintain consistency between training and test preprocessing
- Model ensembling or stacking could be explored for improvement

---

## 📤 Submission Instructions

After running the best-performing model (LightGBM), upload the generated `submission.csv` to the Kaggle portal. Ensure it follows the correct format as provided in `sample_submission.csv`.

---
